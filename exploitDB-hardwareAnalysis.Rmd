---
title: "exploitDB: Analysis of hardware exploits"
author: "Benjamin C. Dean"
date: "04/02/2019"
output:
  html_document: 
    keep_md: yes
    toc: true
    toc_float: true
  html_notebook: default
  pdf_document: default
---

This workbook analyzes data on hardware exploits from the [ExploitDB](https://www.exploit-db.com). As of 4 Feb 2019 there were 1,568 such exploits listed in the database. Each exploit entry contains the following information: date of release, type (e.g. if it affects webapps, OS, etc.), the author, the title (the product the exploit affects and the consequences/impact) and its platform (in this case all are 'hardware' though the ExploitDB contains exploits that affect other platforms).  

Hardware exploits are of interest because they reveal the frequency of known flaws in different types of hardware. They also reveal the probability of certain consequences from exploitation of these flaws. This helps us think through more clearly what flaws could be considered most serious given the frequency with which they are found and what kinds of consequences can/could/should be expected from future discovery. This might help parse out flaws that could be considered defects - in the sense that either they should be identifiable ex ante, given their historical frquency, or given the seriousness of their consequences - from those that should not. We also get an idea of which companies and products are most likely to be subject to litigation for flaws in the future given the frequency of flaws that have appeared in their products in the past. 

```{r setup, include=FALSE}
library(stringr)
library(plyr)
library(tm) # Framework for text mining.
library(dplyr) # Data wrangling, pipe operator %>%().
library(ggplot2) # Plot word frequencies.
library(qdapDictionaries)
library(ggmosaic) # for mosaic plot in ggplot2
library(ggplot2)
library(vcd) # for mosaic plot
library(networkD3) # allows sankey diagrams
library(htmlwidgets) # gives some interactivity
library(dplyr)
library(tidyr)


#library(qdap) # Quantitative discourse analysis of transcripts. 

data <- read.csv("exploitDB.feb19.hardware.csv", sep = ";", header = T, fill = T)
as.data.frame(data)
```
### Cleaning the data
The first step is to import the database. For this analysis, we then need to split the 'title' column in two. This is intended to separate the name of the exploit from its category (i.e. have 'Cisco Firepower Management Center 6.2.2.2 / 6.2.3' in one column then 'Cross-Site Scripting' in another). Fortunately, all the title entries use a '-' to separate the two. We can use the stringr library to help with this.
```{r cars, echo=FALSE, results='hide', message=FALSE}
categories <- as.data.frame(str_split_fixed(data$title, " - ", n = 2)) # make sure there's spaces around the dash otherwise things get messed-up
colnames(categories) <- c("product", "flaw")

# re-attach the two separated to a new df data frame
df <- cbind(data, categories$product)
df <- cbind(df, categories$flaw)

colnames(df) <- c("date", "title", "type", "platform", "author", "product", "flaw")

# clean up the dataset
df$date <- as.Date(data$date, "%d/%m/%Y")
df$title <- NULL # no longer needed
df$platform <- NULL # they are all platform exploits
df$author <- as.character(df$author)
df$product <- as.character(df$product)
df$flaw <- factor(df$flaw) # should help with analysis as a factor

head(df)
```
### Common flaws & their consequences
With the 'flaw' column we can identify the most common consequences from flaws in hardware. The table below lists the top 20 consequences by frequency. Number one is 'Multiple Vulnerabilities' [176 out of 785 entries - or 22%]. The next most common consequences are 'Cross-Site Request Forgery' [71/785 or 9%] and 'Denial of Service' [67/785 or 8.5%].

```{r}
flawNames <- unlist(df$flaw)
flaws <- plyr::count(flawNames) # have to specify plyr to ensure this works
sortedFlaws <- flaws[order(-flaws$freq) , ]

head(sortedFlaws,20)

```

Let's now focus on the 20 most common consequences of exploitation. First we have to construct a data frame with this information. 

```{r}
# NEED TO SUBSET OUR ORIGINAL DF TO HAVE 20 MOST COMMON CONSEQUENCES
# calculate frequencies
tab <- table(df$flaw)
# sort
tab_s <- sort(tab)
# extract 10 most frequent nationalities
top20 <- tail(names(tab_s), 20)
# subset of data frame
d_s <- subset(df, flaw %in% top20)
# order factor levels
d_s$flaw <- factor(d_s$flaw, levels = rev(top20))
```
### Visualizing the analysis
Now let's visualize how each one affects WebApps, Local, Remote, DOS. Webapps seem to be the most prominent across categories - though for obvious reasons different types of Denial of Service (DoS) affect DoS. 

```{r}
ggplot(d_s, aes(x = flaw, fill = as.factor(type))) +
  geom_bar() + 
  theme_classic() +
  theme(axis.text.x=element_text(angle=45,hjust=1))
```

We could also use a mosaic plot to visualize this data. This visualization is even better than the bar chart as it shows the relative proportions of individual exploits for each category of flaw.

```{r}
# see: https://cran.r-project.org/web/packages/ggmosaic/vignettes/ggmosaic.html
ggplot(data = d_s) +
   geom_mosaic(aes(x = product(type, flaw), fill=type), na.rm=TRUE) +
    theme(axis.text.x=element_text(angle=45,hjust=1))

```

It might be interesting to see trends in flaws that reported exploits take advantage of over time. We can do this by using a point chart that takes the date on the x-axis and the type of flaw on the y-axis. 

There's some interesting trends here. 'Multiple cross-site request forgery vulnerabilities' stopped appearing in reported expoits as of around 2015. So too 'remote denial of service' has also declined in frequency. By contrast, there has been an increase in the frequency of 'cross-site request forgery', which might explain the deline in the reporting of the 'multiple' kinds. 'Cross-site scripting' has emerged as a particularly frequent flaw recently. 

```{r} 
ggplot(data = d_s, aes(x=date, y=flaw)) +
  geom_point(stat="identity")
```
### Analysing the products involved
Let's see what we can do with the data on the products and brands as they relate to the flaws. 

A great thing about the 'product' column in our df is that the first word is always the brand/company that made the device. We could simply filter out everything after that first word then see where we land in terms of distribution.

The way to do it might be to use the first 'space' as the cut-off point of the strings.

```{r, echo=FALSE, message=FALSE}
# DATA CLEANING
d_p <- as.data.frame(str_split_fixed(df$product, " ", n = 2)) 
colnames(d_p) <- c("brand", "model")

# re-attach the two separated to a new df data frame
df <- cbind(df, d_p$brand)
df <- cbind(df, d_p$model)

# do the column names again for df
colnames(df) <- c("date", "type", "author", "product", "flaw", "brand", "model")

# filter down so that only the top 20 brands are collected
# calculate frequencies
tab <- table(df$brand)
# sort
tab_s <- sort(tab)
# extract 10 most frequent nationalities
top20 <- tail(names(tab_s), 20)
# subset of data frame
d_p <- subset(df, brand %in% top20)
# order factor levels
d_p$brand <- factor(d_p$brand, levels = rev(top20))
```

Below are the top 20 brands by the number of exploits that affect them. D-link and Cisco come out on top - though this might be because they have more products on the market than other brands. 

```{r}
ggplot(d_p, aes(x = brand, fill = as.factor(type))) +
  geom_bar() + 
  theme_classic() +
  theme(axis.text.x=element_text(angle=45,hjust=1))
```

We could try and do a Sankey chart to see which companies are most associated with certain categories of flaws. Below is an attempt though it does not visualize well. 

_NB: if anyone has any ideas on how this could be cleaned up, please let it be known._

```{r, echo=FALSE, message=FALSE}
# for help: https://stackoverflow.com/questions/44132423/creating-a-sankey-diagram-using-networkd3-package-in-r

sankey <- data.frame(d_p$brand, d_p$flaw)
names(sankey) <- c("brand", "flaw")


links <-
  sankey %>%
  mutate(row = row_number()) %>%
  gather('column', 'source', -row) %>%
  mutate(column = match(column, names(sankey))) %>%
  group_by(row) %>%
  arrange(column) %>%
  mutate(target = lead(source)) %>%
  ungroup() %>%
  filter(!is.na(target))

nodes <- data.frame(name = unique(c(links$source, links$target)))

links$source <- match(links$source, nodes$name) - 1
links$target <- match(links$target, nodes$name) - 1
links$value <- 1

nodes$name <- sub('_[0-9]+$', '', nodes$name)

sankeyNetwork(Links = links, Nodes = nodes, Source = 'source',
              Target = 'target', Value = 'value', NodeID = 'name', 
              fontSize = 12, nodeWidth = 30)



```

We can do a similar analysis on the products to see which are the naughty ones. The issue here is that the brands and the products themselves are mixed together. We might do better by running a frequency analysis of individual words rather than whole phrases. 

```{r}
productNames <- unlist(df$product)
products <- plyr::count(productNames)
sortedProducts <- products[order(-products$freq) , ]

head(sortedProducts,20)

```

### Next steps

Time and interest permitting - here are some elements that I'd like to explore in the future:

- parse out the products from the brands

- see if we can link products to types of exploits/consequences

- which of the consequences are 'most serious', according to which criteria? 
